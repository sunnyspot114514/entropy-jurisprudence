\documentclass[11pt]{article}

% ==========================================
% PACKAGES
% ==========================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{natbib}
\usepackage{geometry}
\geometry{margin=1in}

% ==========================================
% TITLE
% ==========================================
\title{Entropy Jurisprudence: Auditing Normative Consistency in Large Language Models}
\author{Chen, Xiwei}
\date{\today}

\begin{document}

\maketitle

% ==========================================
% ABSTRACT
% ==========================================
\begin{abstract}
Current evaluations of large language models (LLMs) focus on whether models reach morally correct conclusions, but neglect whether they reason consistently under normative pressure. We introduce \textbf{Entropy Jurisprudence}, a procedural audit framework that tests whether LLMs can faithfully execute a formal moral rule once committed to it. Our framework uses a minimal harm formula ($E = H \times R$) to create auditable commitments, then measures parameter stability and verdict consistency across repeated trials. Experiments on six open-weight models (8B parameter class) reveal a striking pattern: models converge on irreversibility estimates (Kruskal-Wallis $p=0.91$) but diverge significantly on verdict logic. We identify cases where models exhibit high rationalization indices (RI $> 30$), indicating post-hoc justification of intuitive verdicts. Our framework provides a reproducible methodology for detecting procedural drift in AI moral reasoning.
\end{abstract}

% ==========================================
% 1. INTRODUCTION
% ==========================================
\section{Introduction}

As large language models are increasingly deployed in high-stakes domains—legal analysis, medical triage, autonomous systems—the question of \textit{how} they reason becomes as important as \textit{what} they conclude. A model that reaches the ``correct'' verdict through inconsistent reasoning may be more dangerous than one that is consistently wrong, because its failures are unpredictable.

Existing moral reasoning benchmarks \citep{hendrycks2021ethics} evaluate whether models align with human moral judgments. However, they do not test whether models can \textit{follow their own stated rules} when the outcome conflicts with intuition. This gap matters: a model that commits to a formal decision procedure but abandons it under pressure exhibits a form of \textbf{procedural infidelity} that undermines trust.

We introduce \textbf{Entropy Jurisprudence}, a framework for auditing normative consistency in LLMs. Our approach:
\begin{enumerate}
    \item Commits the model to a minimal, deterministic moral rule
    \item Presents cases designed to create tension between rule-following and intuition
    \item Measures whether the model executes the rule faithfully or rationalizes a preferred verdict
\end{enumerate}

Our key contributions are:
\begin{itemize}
    \item A formal framework for procedural auditing of moral reasoning
    \item Metrics for detecting post-hoc rationalization (Rationalization Index)
    \item Empirical evidence that models converge on parameter estimates but diverge on verdict logic
    \item A reproducible experimental methodology accessible on consumer hardware
\end{itemize}

% ==========================================
% 2. RELATED WORK
% ==========================================
\section{Related Work}

\subsection{Moral Reasoning Evaluation}
The ETHICS benchmark \citep{hendrycks2021ethics} evaluates models on commonsense moral judgments. MoralBench and similar efforts test value alignment. These approaches measure \textit{outcome correctness}—whether the model's conclusion matches human consensus. Our work differs by measuring \textit{procedural fidelity}—whether the model honors its stated reasoning process.

\subsection{Reasoning Faithfulness}
Recent work has questioned whether chain-of-thought explanations reflect actual model reasoning \citep{turpin2023language}. Models may generate plausible-sounding justifications that do not correspond to their decision process. Our Rationalization Index directly measures this phenomenon in moral contexts.

% ==========================================
% 3. METHOD
% ==========================================
\section{Method}

\subsection{The Formal Rule}
We define a minimal harm calculus:
\begin{equation}
E = H \times R
\end{equation}
where:
\begin{itemize}
    \item $E$ = Effective Harm (final score)
    \item $H$ = Base Harm (immediate negative impact, $[0,10]$)
    \item $R$ = Irreversibility coefficient:
    \begin{itemize}
        \item $0.1$ = Reversible (e.g., insured money)
        \item $1.0$ = Difficult to repair
        \item $2.0$ = Permanent (death, extinction)
    \end{itemize}
\end{itemize}

The verdict rule is deterministic:
\begin{equation}
\text{Verdict} = \begin{cases} \text{Not Guilty} & \text{if } I > E \\ \text{Guilty} & \text{otherwise} \end{cases}
\end{equation}
where $I$ = Intent (moral goodness of motive, $[0,10]$).

\subsection{Metrics}
\textbf{Rationalization Index (RI):}
\begin{equation}
RI = \frac{\sigma_R}{\sigma_V + \epsilon}
\end{equation}
where $\sigma_R$ is the standard deviation of R-values and $\sigma_V$ is the standard deviation of verdicts (binary). High RI indicates stable verdicts with unstable reasoning—a signature of rationalization.

\subsection{Procedural Audit Algorithm}

\begin{algorithm}[H]
\caption{Procedural Audit}
\begin{algorithmic}[1]
\Require Intent $I$, Base Harm $H$, Irreversibility $R$, Model Verdict $v$
\Ensure Audit Status $\in \{\textsc{Executed}, \textsc{Rationalized}\}$
\State $E \gets H \times R$ \Comment{Compute Effective Harm}
\If{$I > E$}
    \State $v^* \gets \textsc{Not\_Guilty}$
\Else
    \State $v^* \gets \textsc{Guilty}$
\EndIf
\If{$v = v^*$}
    \State \Return \textsc{Executed} \Comment{Model followed the rule}
\Else
    \State \Return \textsc{Rationalized} \Comment{Model violated the rule}
\EndIf
\end{algorithmic}
\end{algorithm}

% ==========================================
% 4. RESULTS
% ==========================================
\section{Results}

\subsection{Model Summary}
We evaluated six open-weight models across four boundary-stress scenarios (30 iterations each, N=720 total trials). Table~\ref{tab:model_summary} summarizes procedural fidelity metrics.

\begin{table}[h]
\centering
\caption{Model-level procedural fidelity metrics}
\label{tab:model_summary}
\begin{tabular}{lcccc}
\toprule
Model & Executed\% & Rationalized\% & R-Hallucinated\% & Guilty\% \\
\midrule
Qwen3:8b & \textbf{92.5} & 5.0 & 1.7 & 55.0 \\
Mistral:7b & 88.3 & 9.2 & \textbf{0.0} & 70.0 \\
Llama3:8b & 85.0 & 12.5 & 3.3 & 61.7 \\
DeepSeek-R1:8b & 81.7 & 6.7 & 11.7 & 42.5 \\
Gemma3:4b & 67.5 & \textbf{32.5} & 0.8 & \textbf{97.5} \\
Phi3:3.8b & 50.8 & 30.8 & \textbf{30.0} & 65.0 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Parameter Convergence}
Despite architectural differences, models converge on irreversibility estimates. A Kruskal-Wallis test reveals no significant difference in R-value distributions across models ($H=2.245$, $p=0.81$). This suggests models share a common understanding of \textit{what} constitutes irreversible harm.

\subsection{Verdict Divergence}
While parameter estimates converge, verdict patterns diverge dramatically. Gemma3 exhibits ``moral rigidity''—97.5\% Guilty rate regardless of case parameters. In contrast, DeepSeek shows the lowest Guilty rate (42.5\%), correctly identifying reversible scenarios.

\subsection{Rationalization Detection}
Table~\ref{tab:ri} shows per-case Rationalization Index values. The Ancient\_Tree case (high irreversibility) triggers extreme rationalization in Llama3 (RI=328.59), indicating the model maintains a fixed verdict while wildly varying its parameter justifications.

\begin{table}[h]
\centering
\caption{Rationalization Index by model and case}
\label{tab:ri}
\begin{tabular}{lcccc}
\toprule
Model & Bank\_Hacker & Ancient\_Tree & Cancer\_Fungus & Digital\_Hostage \\
\midrule
DeepSeek-R1 & 9.48 & 26.51 & 2.52 & 8.59 \\
Qwen3 & 0.00 & 0.00 & 0.00 & 2.93 \\
Gemma3 & 2.48 & 0.00 & 9.97 & 3.66 \\
Llama3 & 4.49 & \textbf{328.59} & 1.50 & 1.71 \\
Mistral & 0.00 & 0.00 & 1.68 & 1.27 \\
Phi3 & 4.37 & 5.97 & 12.16 & 7.55 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Temperature Ablation}
Temperature ablation (T-ANBS) on two representative models reveals stability differences. Qwen3 maintains low variance across all temperatures (Std $< 2$), while DeepSeek shows high variance on high-R cases (Std up to 29.29 at T=0.9 on Ancient\_Tree). This suggests procedural stability is model-dependent, not purely a function of decoding temperature.

% ==========================================
% 5. DISCUSSION
% ==========================================
\section{Discussion}

\subsection{Implications for Agent Safety}
Our findings have direct implications for deploying LLM-based agents in high-stakes domains:

\textbf{Security}: Rationalization represents a rule-bypass vulnerability. An agent that can manipulate its own parameters to justify a predetermined conclusion may circumvent safety constraints designed to prevent harmful actions.

\textbf{Alignment}: Models may ``know'' the correct rule but fail to follow it. Gemma3's 97.5\% Guilty rate, despite varying case parameters, suggests alignment to a prior (``always reject'') rather than faithful rule execution.

\textbf{Deployment}: Procedural audits should precede deployment in irreversible-action contexts. Our framework provides a lightweight diagnostic that can be run on consumer hardware.

\subsection{Model Behavioral Patterns}
We observe three distinct behavioral archetypes:

\textbf{Procedural Executors} (Qwen3, Mistral): High execution rates ($>88\%$), low rationalization (RI $\approx 0$ on multiple cases). These models follow the committed rule even when outcomes are counterintuitive.

\textbf{Moral Rigidifiers} (Gemma3): High verdict stability but through prior bias, not rule execution. The 97.5\% Guilty rate suggests safety fine-tuning may have induced ``always reject'' behavior.

\textbf{Expressive Rationalizers} (Llama3, DeepSeek): Moderate execution rates with high parameter variance. These models appear to ``feel'' the moral weight of cases and adjust parameters accordingly—a form of post-hoc justification.

\subsection{Limitations}
Our framework has several limitations. The formula $E = H \times R$ is a minimal testable formulation; alternative forms were not systematically compared. R-value discretization sacrifices granularity for testability. No human baseline was collected, so we cannot determine if observed patterns are LLM-specific. The four test cases may not represent typical deployment scenarios. Finally, ``rationalization'' is operationally defined—models penalized for parameter drift may be exhibiting legitimate moral sensitivity.

% ==========================================
% 6. CONCLUSION
% ==========================================
\section{Conclusion}

We introduced Entropy Jurisprudence, a procedural audit framework for evaluating whether LLMs faithfully execute moral rules or rationalize around them. Our experiments on six open-weight models reveal:

\begin{enumerate}
    \item \textbf{Parameter convergence}: Models agree on irreversibility estimates (Kruskal-Wallis $p=0.81$), suggesting shared harm perception.
    \item \textbf{Verdict divergence}: Despite parameter agreement, verdict patterns vary dramatically (42.5\%--97.5\% Guilty rates).
    \item \textbf{Systematic rationalization}: High-irreversibility cases trigger extreme rationalization (RI up to 328.59), with models maintaining fixed verdicts while varying justifications.
    \item \textbf{Model-dependent stability}: Some models (Qwen3, Mistral) are temperature-immune; others (DeepSeek, Llama3) show high variance under boundary pressure.
\end{enumerate}

These findings suggest that procedural audits are necessary before deploying LLM-based agents in contexts involving irreversible actions. Entropy Jurisprudence provides a minimal, reproducible framework for such audits, accessible on consumer hardware.

Future work should establish human baselines, test full agent pipelines with tool use, and scale to larger models. The framework's simplicity is both a limitation and a feature: it creates an auditable commitment that exposes procedural drift without requiring complex ethical theory.

% ==========================================
% ACKNOWLEDGMENTS
% ==========================================
\section*{Acknowledgments}
We used multiple AI assistants to support code development and manuscript preparation. All experimental design, execution, and validation were performed by the author.

% ==========================================
% REFERENCES
% ==========================================
\bibliographystyle{plainnat}
\begin{thebibliography}{9}

\bibitem[Hendrycks et al.(2021)]{hendrycks2021ethics}
Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt.
\newblock Aligning AI with shared human values.
\newblock In \textit{ICLR}, 2021.

\bibitem[Turpin et al.(2023)]{turpin2023language}
Miles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman.
\newblock Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting.
\newblock \textit{arXiv preprint arXiv:2305.04388}, 2023.

\end{thebibliography}

\end{document}
