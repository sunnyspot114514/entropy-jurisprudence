\documentclass[11pt]{article}

% ==========================================
% PACKAGES
% ==========================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{natbib}
\usepackage{geometry}
\geometry{margin=1in}

% ==========================================
% TITLE
% ==========================================
\title{Entropy Jurisprudence: Auditing Normative Consistency in Large Language Models}
\author{Chen, Xiwei}
\date{\today}

\begin{document}

\maketitle

% ==========================================
% ABSTRACT
% ==========================================
\begin{abstract}
Current evaluations of large language models (LLMs) focus on whether models reach morally correct conclusions, but neglect whether they reason consistently under normative pressure. We introduce \textbf{Entropy Jurisprudence}, a procedural audit framework that tests whether LLMs can faithfully execute a formal moral rule once committed to it. Our framework uses a minimal harm formula ($E = H \times R$) to create auditable commitments, then measures parameter stability and verdict consistency across repeated trials. Experiments on six open-weight models (8B parameter class) reveal a striking pattern: models converge on irreversibility estimates (Kruskal-Wallis $p=0.91$) but diverge significantly on verdict logic. We identify cases where models exhibit high rationalization indices (RI $> 30$), indicating post-hoc justification of intuitive verdicts. Our framework provides a reproducible methodology for detecting procedural drift in AI moral reasoning.
\end{abstract}

% ==========================================
% 1. INTRODUCTION
% ==========================================
\section{Introduction}

As large language models are increasingly deployed in high-stakes domains—legal analysis, medical triage, autonomous systems—the question of \textit{how} they reason becomes as important as \textit{what} they conclude. A model that reaches the ``correct'' verdict through inconsistent reasoning may be more dangerous than one that is consistently wrong, because its failures are unpredictable.

Existing moral reasoning benchmarks \citep{hendrycks2021ethics} evaluate whether models align with human moral judgments. However, they do not test whether models can \textit{follow their own stated rules} when the outcome conflicts with intuition. This gap matters: a model that commits to a formal decision procedure but abandons it under pressure exhibits a form of \textbf{procedural infidelity} that undermines trust.

We introduce \textbf{Entropy Jurisprudence}, a framework for auditing normative consistency in LLMs. Our approach:
\begin{enumerate}
    \item Commits the model to a minimal, deterministic moral rule
    \item Presents cases designed to create tension between rule-following and intuition
    \item Measures whether the model executes the rule faithfully or rationalizes a preferred verdict
\end{enumerate}

Our key contributions are:
\begin{itemize}
    \item A formal framework for procedural auditing of moral reasoning
    \item Metrics for detecting post-hoc rationalization (Rationalization Index)
    \item Empirical evidence that models converge on parameter estimates but diverge on verdict logic
    \item A reproducible experimental methodology accessible on consumer hardware
\end{itemize}

% ==========================================
% 2. RELATED WORK
% ==========================================
\section{Related Work}

\subsection{Moral Reasoning Evaluation}
The ETHICS benchmark \citep{hendrycks2021ethics} evaluates models on commonsense moral judgments. MoralBench and similar efforts test value alignment. These approaches measure \textit{outcome correctness}—whether the model's conclusion matches human consensus. Our work differs by measuring \textit{procedural fidelity}—whether the model honors its stated reasoning process.

\subsection{Reasoning Faithfulness}
Recent work has questioned whether chain-of-thought explanations reflect actual model reasoning \citep{turpin2023language}. Models may generate plausible-sounding justifications that do not correspond to their decision process. Our Rationalization Index directly measures this phenomenon in moral contexts.

% ==========================================
% 3. METHOD
% ==========================================
\section{Method}

\subsection{The Formal Rule}
We define a minimal harm calculus:
\begin{equation}
E = H \times R
\end{equation}
where:
\begin{itemize}
    \item $E$ = Effective Harm (final score)
    \item $H$ = Base Harm (immediate negative impact, $[0,10]$)
    \item $R$ = Irreversibility coefficient:
    \begin{itemize}
        \item $0.1$ = Reversible (e.g., insured money)
        \item $1.0$ = Difficult to repair
        \item $2.0$ = Permanent (death, extinction)
    \end{itemize}
\end{itemize}

The verdict rule is deterministic:
\begin{equation}
\text{Verdict} = \begin{cases} \text{Not Guilty} & \text{if } I > E \\ \text{Guilty} & \text{otherwise} \end{cases}
\end{equation}
where $I$ = Intent (moral goodness of motive, $[0,10]$).

\subsection{Metrics}
\textbf{Rationalization Index (RI):}
\begin{equation}
RI = \frac{\sigma_R}{\sigma_V + \epsilon}
\end{equation}
where $\sigma_R$ is the standard deviation of R-values and $\sigma_V$ is the standard deviation of verdicts (binary). High RI indicates stable verdicts with unstable reasoning—a signature of rationalization.

% ==========================================
% 4. RESULTS (PLACEHOLDER)
% ==========================================
\section{Results}

\textit{[Results to be filled in after experiments]}

% ==========================================
% 5. DISCUSSION
% ==========================================
\section{Discussion}

\textit{[Discussion to be written]}

% ==========================================
% 6. CONCLUSION
% ==========================================
\section{Conclusion}

\textit{[Conclusion to be written]}

% ==========================================
% REFERENCES
% ==========================================
\bibliographystyle{plainnat}
\begin{thebibliography}{9}

\bibitem[Hendrycks et al.(2021)]{hendrycks2021ethics}
Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt.
\newblock Aligning AI with shared human values.
\newblock In \textit{ICLR}, 2021.

\bibitem[Turpin et al.(2023)]{turpin2023language}
Miles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman.
\newblock Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting.
\newblock \textit{arXiv preprint arXiv:2305.04388}, 2023.

\end{thebibliography}

\end{document}
